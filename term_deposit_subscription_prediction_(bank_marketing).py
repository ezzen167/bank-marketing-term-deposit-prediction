# -*- coding: utf-8 -*-
"""Term Deposit Subscription Prediction (Bank Marketing)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14el3c5WKLYotpLR9ppmTP5brLZEfhIFL
"""

# Commented out IPython magic to ensure Python compatibility.
# Cell 1: Install required packages (run once)
!pip install -q shap imbalanced-learn

# Imports
import warnings
warnings.filterwarnings('ignore')

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_auc_score, roc_curve

from imblearn.over_sampling import SMOTE
import shap
import joblib

print("Libraries loaded. numpy/pandas/sklearn/shap/imblearn/joblib ready.")

"""Upload dataset"""

import pandas as pd

df = pd.read_csv("/content/bank-full.csv", sep=";")
print(df.head())
print(df.info())
# Cell 3: Load dataset (try semicolon sep first)

print("Rows,Cols:", df.shape)
display(df.head())
print("\nInfo:")
print(df.info())
print("\nTarget distribution (y):")
print(df['y'].value_counts())
print("\nPercent:")
print(df['y'].value_counts(normalize=True)*100)

"""Small EDA plots"""

# Cell 4: Quick EDA visuals
plt.figure(figsize=(6,4))
sns.countplot(x='y', data=df)
plt.title('Target counts (y)')
plt.show()

plt.figure(figsize=(8,4))
plt.hist(df['balance'], bins=50)
plt.title('Balance distribution')
plt.xlabel('Balance')
plt.show()

"""RELOAD & CLEAN PIPELINE"""

#  RELOAD & CLEAN PIPELINE
import os
import pandas as pd
import numpy as np

# 1) Try to use existing data_file variable if present, else default path
try:
    data_file  # if defined earlier in notebook
except NameError:
    data_file = "/content/bank-full.csv"   # change if your file has different name/path

# 2) Reload raw CSV (try ; separator first)
print("Loading file:", data_file)
try:
    raw = pd.read_csv(data_file, sep=';')
    print("Loaded with semicolon separator.")
except Exception as e:
    raw = pd.read_csv(data_file)
    print("Loaded with comma separator.")

print("Raw shape:", raw.shape)
print("\nRaw columns:", raw.columns.tolist())

# 3) Inspect raw y column values
print("\nUnique values in raw['y'] (first 50):")
print(raw['y'].unique()[:50])
print("raw['y'] dtype:", raw['y'].dtype)
print("Nulls in raw['y']:", raw['y'].isnull().sum())

# 4) Clean target column in raw dataframe robustly
raw = raw.copy()
# If values are strings like 'yes'/'no', normalize; if numeric 0/1 leave as is
if raw['y'].dtype == object or raw['y'].dtype == 'string':
    raw['y_clean'] = raw['y'].astype(str).str.strip().str.lower()
    # Map yes/no to 1/0; keep other values NaN for inspection
    raw['y'] = raw['y_clean'].map({'no':0, 'yes':1})
    raw.drop(columns=['y_clean'], inplace=True)
else:
    # numeric type: ensure only 0/1 and convert to int
    if set(np.unique(raw['y'])) <= {0,1}:
        raw['y'] = raw['y'].astype(int)
    else:
        # if numeric but contains weird values, print them
        print("Warning: raw['y'] has numeric values outside {0,1}:", np.unique(raw['y']))
        # try a safe convert: treat non-zero as 1
        raw['y'] = (raw['y'] != 0).astype(int)

print("After cleaning, unique y:", raw['y'].unique(), "Nulls:", raw['y'].isnull().sum())

# 5) If any nulls remain in y, show some rows to inspect and drop them
if raw['y'].isnull().sum() > 0:
    print("\nShowing up to 10 rows with invalid/missing y for inspection:")
    display(raw[raw['y'].isnull()].head(10))
    print("Dropping rows with missing target (safe approach).")
    raw = raw[raw['y'].notnull()].copy()
    raw['y'] = raw['y'].astype(int)

print("\nShape after dropping invalid targets (if any):", raw.shape)

# 6) Now encode categoricals safely (exclude y)
cat_cols = raw.select_dtypes(include=['object', 'string']).columns.tolist()
if 'y' in cat_cols:
    cat_cols.remove('y')
print("\nCategorical columns to encode:", cat_cols)

df_encoded = pd.get_dummies(raw, columns=cat_cols, drop_first=True)
print("Encoded shape:", df_encoded.shape)

# 7) Final sanity checks
print("\nAny NaNs in encoded y?:", df_encoded['y'].isnull().sum())
print("dtype(encoded y):", df_encoded['y'].dtype)
print("Unique(encoded y):", np.unique(df_encoded['y'])[:10])
print("\nAny NaNs in X?:", df_encoded.drop('y', axis=1).isnull().any().any())

# 8) Create X,y ready for modeling
X = df_encoded.drop('y', axis=1).reset_index(drop=True)
y = df_encoded['y'].astype(int).reset_index(drop=True)

print("\nFinal shapes -> X:", X.shape, " y:", y.shape)
print("Final target distribution (%):")
print(y.value_counts(normalize=True)*100)

"""Train/Test Split + Scaling"""

# === Train/Test Split + Scaling ===
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Train/Test split (stratify y taake imbalance preserve ho)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Train:", X_train.shape, " Test:", X_test.shape)
print("Class balance in train:", y_train.value_counts(normalize=True)*100)

# Scale numeric columns
num_cols = ['age','balance','day','duration','campaign','pdays','previous']
scaler = StandardScaler()
X_train[num_cols] = scaler.fit_transform(X_train[num_cols])
X_test[num_cols] = scaler.transform(X_test[num_cols])

"""SMOTE (imbalance fix)"""

# === Handle imbalance with SMOTE ===
from imblearn.over_sampling import SMOTE

sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

print("Before SMOTE:", y_train.value_counts())
print("After SMOTE:", y_train_res.value_counts())

"""1) Logistic Regression"""

from sklearn.linear_model import LogisticRegression

# Logistic Regression model
lr = LogisticRegression(max_iter=1000, random_state=42)
lr.fit(X_train_res, y_train_res)

# Predictions
y_pred_lr = lr.predict(X_test)
y_proba_lr = lr.predict_proba(X_test)[:,1]

"""2) Random Forest + Evaluation Function"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Random Forest model
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train_res, y_train_res)

y_pred_rf = rf.predict(X_test)
y_proba_rf = rf.predict_proba(X_test)[:,1]

# Evaluation helper function
def evaluate_model(y_test, y_pred, y_proba, name):
    print(f"\n--- {name} ---")
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))
    print("F1 Score:", f1_score(y_test, y_pred))
    print("ROC AUC:", roc_auc_score(y_test, y_proba))

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    plt.figure(figsize=(6,4))
    plt.plot(fpr, tpr, label=f"{name} (AUC={roc_auc_score(y_test,y_proba):.3f})")
    plt.plot([0,1],[0,1],'k--')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"ROC Curve - {name}")
    plt.legend()
    plt.show()

# Evaluate both models
evaluate_model(y_test, y_pred_lr, y_proba_lr, "Logistic Regression")
evaluate_model(y_test, y_pred_rf, y_proba_rf, "Random Forest")

"""1) Ensure exact same feature columns & order that RF was trained on"""

# features RF was trained on:
trained_cols = X_train_res.columns.tolist()
print("Num features RF trained on:", len(trained_cols))

# features in our SHAP numeric sample
sample_cols = X_sample_num.columns.tolist()
print("Num features in X_sample_num:", len(sample_cols))

# compare sets and order
missing_in_sample = [c for c in trained_cols if c not in sample_cols]
extra_in_sample = [c for c in sample_cols if c not in trained_cols]
print("Missing in sample (should be none):", missing_in_sample[:10])
print("Extra in sample (should be none):", extra_in_sample[:10])

# If order differs, reorder X_sample_num to match trained order:
X_sample_aligned = X_sample_num.reindex(columns=trained_cols)
print("Any NaNs after reindex (should be False):", X_sample_aligned.isnull().any().any())
# If any NaNs appear because a column was missing, inspect above lists.

"""# Robust additivity diagnostic + plotting for binary classification SHAP outputs"""

# Robust additivity diagnostic + plotting for binary classification SHAP outputs
import numpy as np
import shap

# sv: Shapley object from explainer(X_sample_num)
# X_sample_num: aligned numeric sample (n_samples, n_features)
# rf: trained RandomForestClassifier

print("sv.values.shape:", np.shape(sv.values))
print("sv.base_values shape/type:", type(sv.base_values), np.shape(sv.base_values))
print("X_sample_num.shape:", X_sample_num.shape)

# Compute shap_sums properly depending on sv.values shape
vals = np.array(sv.values)   # convert to numpy array for safe indexing
# sv.values might be:
# - shape (n_samples, n_features)  -> binary/single-output
# - shape (n_samples, n_outputs, n_features) -> multi-output (per class)
# Handle both:

if vals.ndim == 2:
    # simple case: (n_samples, n_features)
    shap_sums = vals.sum(axis=1)                 # shape (n_samples,)
    base_vals = np.array(sv.base_values)        # scalar or length n_samples
    # ensure base_vals is 1d of length n_samples or scalar
    if np.shape(base_vals) == ():
        base_vals = np.repeat(base_vals, len(shap_sums))
    elif len(base_vals) == len(shap_sums):
        base_vals = np.array(base_vals)
    else:
        # unexpected shape
        base_vals = np.array(base_vals).reshape(-1)[:len(shap_sums)]
    model_probs = rf.predict_proba(X_sample_num)[:,1]
    approx = base_vals + shap_sums
    diffs = model_probs - approx

else:
    # multi-output case: (n_samples, n_outputs, n_features)
    # We want the explanation for class 1 (positive class). Usually class index 1
    # shap_sums_per_class shape => (n_samples, n_outputs)
    shap_sums_per_class = vals.sum(axis=2)   # sum across features -> shape (n_samples, n_outputs)
    base_vals_arr = np.array(sv.base_values) # may be shape (n_outputs,) or (n_samples, n_outputs)
    n_samples = X_sample_num.shape[0]
    # Make base_vals_arr shape (n_samples, n_outputs)
    if base_vals_arr.ndim == 1:
        base_vals_arr = np.tile(base_vals_arr.reshape(1,-1), (n_samples,1))
    elif base_vals_arr.shape[0] == n_samples:
        base_vals_arr = np.array(base_vals_arr)
    else:
        # fallback: broadcast first row
        base_vals_arr = np.tile(base_vals_arr.reshape(1,-1), (n_samples,1))
    # Select class index for positive class
    pos_class_index = 1
    if shap_sums_per_class.shape[1] <= pos_class_index:
        # if no class-1, pick last class
        pos_class_index = shap_sums_per_class.shape[1]-1
    shap_sums_class1 = shap_sums_per_class[:, pos_class_index]   # (n_samples,)
    base_class1 = base_vals_arr[:, pos_class_index]              # (n_samples,)
    approx = base_class1 + shap_sums_class1
    model_probs = rf.predict_proba(X_sample_num)[:,1]
    diffs = model_probs - approx

# Print diagnostics
print("\nAdditivity diagnostics (per sample):")
for i in range(len(approx)):
    print(f"sample {i}: model_prob={model_probs[i]:.6f}, base+sum_shap={approx[i]:.6f}, diff={diffs[i]:.6e}")

# Acceptable threshold? show summary
print("\nSummary of diffs: min, mean, max:", diffs.min(), diffs.mean(), diffs.max())

# If diffs are small (like <1e-3) it's fine. If large, consider check_additivity=Fa_

"""# Run this block to plot SHAP waterfall for class-1 explanations"""

# Run this block to plot SHAP waterfall for class-1 explanations
import shap
shap.initjs()

# decide which class index is positive (usually 1)
pos_class_index = 1

# If sv has shape (n_samples, n_features, n_classes) then use sv[:, pos_class_index]
import numpy as np
print("sv.values.shape:", np.shape(sv.values))

# Per-sample waterfall for class 1
for i in range(len(X_sample_num)):
    try:
        print(f"\n--- Sample {i} — True: {y_sample[i] if 'y_sample' in globals() else 'NA'} — Pred prob: {rf.predict_proba(X_sample_num.iloc[[i]])[0,1]:.3f} ---")
        # plot the class-1 explanation for sample i
        shap.plots.waterfall(sv[i, pos_class_index])
    except Exception as e:
        print("Could not plot waterfall for sample", i, "error:", e)
        # fallback: try to extract values and print top features
        try:
            vals = sv.values[i, pos_class_index]
            top_idx = np.argsort(-np.abs(vals))[:8]
            print("Top features (fallback):")
            for j in top_idx:
                print(X_sample_num.columns[j], vals[j])
        except Exception as e2:
            print("Fallback also failed:", e2)

# Global summary (bar) for class-1 across the samples
try:
    # Build an array of class-1 shap values for plotting
    if np.ndim(sv.values) == 3:
        shap_vals_class1 = sv.values[:, :, pos_class_index]
    else:
        # if it's 2D already, use sv.values
        shap_vals_class1 = sv.values
    shap.summary_plot(shap_vals_class1, X_sample_num, plot_type="bar")
except Exception as e:
    print("Summary plot failed:", e)

"""#  Make proper shap.Explanation objects per sample and plot waterfall (class-1)"""

# Make proper shap.Explanation objects per sample and plot waterfall (class-1)
import shap
import numpy as np
from IPython.display import display

pos_class_index = 1   # choose positive class

# sv is the shap.Explanation object from explainer(X_sample_num)
# Check shapes
print("sv.values.shape:", np.shape(sv.values))
print("sv.base_values shape:", np.shape(sv.base_values))
print("X_sample_num.shape:", X_sample_num.shape)

# For each sample build a shap.Explanation for class-1 and plot
shap.initjs()
for i in range(len(X_sample_num)):
    try:
        # Extract class-1 values and base for sample i
        if sv.values.ndim == 3:
            vals = sv.values[i, :, pos_class_index]            # shape (n_features,)
            base = sv.base_values[i, pos_class_index] if np.shape(sv.base_values)[0] == len(X_sample_num) else sv.base_values[pos_class_index]
        else:
            vals = sv.values[i, :]                             # 2D case
            base = sv.base_values[i] if np.shape(sv.base_values)[0] == len(X_sample_num) else sv.base_values

        # Build shap.Explanation object
        expl = shap.Explanation(values=vals,
                               base_values=base,
                               data=X_sample_num.iloc[i].values,
                               feature_names=X_sample_num.columns.tolist())

        print(f"\n--- Sample {i} — True: {y_sample[i] if 'y_sample' in globals() else 'NA'} — Pred prob: {rf.predict_proba(X_sample_num.iloc[[i]])[0,1]:.3f} ---")
        # Waterfall (matplotlib friendly)
        shap.plots.waterfall(expl)
    except Exception as e:
        print("Waterfall plotting failed for sample", i, ":", e)
        # fallback: print top contributing features
        try:
            top_idx = np.argsort(-np.abs(vals))[:8]
            print("Top features (fallback):")
            for j in top_idx:
                print(X_sample_num.columns[j], vals[j])
        except Exception as e2:
            print("Fallback also failed:", e2)

# Global bar summary: use class-1 values from sv
try:
    if sv.values.ndim == 3:
        class1_vals = sv.values[:, :, pos_class_index]   # shape (n_samples, n_features)
    else:
        class1_vals = sv.values
    shap.summary_plot(class1_vals, X_sample_num, plot_type="bar")
except Exception as e:
    print("Global summary plot failed:", e)